# Multi-Agent Security Orchestration Platform

This project deploys a comprehensive, multi-agent system on Google Cloud designed for security orchestration. It features a web-based chat interface where an operator can manage a security incident workflow. A central **Orchestrator Agent** delegates tasks like threat intelligence, malware analysis, and post-mortem reporting to a team of specialized, remote AI agents running on Vertex AI.

The system includes a complete backend for file processing, allowing users to upload suspicious files, which are then automatically disassembled and hashed for analysis by the agent network.



![Architecture Diagram](https://github.com/Eran-T/SOC-MAS/blob/main/a2a-sec%20deploy/assets/SOC%20MAS.png)


## Architecture

The platform consists of four main component groups deployed across Google Cloud:

1.  **Frontend (Cloud Run):**
    * A FastAPI and Socket.IO application (`a2a_demo_front/front`) serves the "CYBERMODE" web interface.
    * It manages chat sessions, sends user messages to the Orchestrator Agent via A2A protocol, and receives real-time event updates from agents via a dedicated `/event` HTTP endpoint to display in the UI.
    * Deployed as a containerized service on Cloud Run for scalability and ease of management.

2.  **Orchestrator Agent (Vertex AI Reasoning Engine):**
    * The "brain" of the operation, deployed as the `chat_agent` (`a2a_demo_front/agents/chat_agent`).
    * Runs on Vertex AI Reasoning Engines.
    * Follows a strict, workflow-driven instruction set to coordinate the security response.
    * Delegates tasks to specialized agents based on capability requirements.
    * Manages file handling by generating signed GCS URLs for user uploads and agent downloads via its tools (`create_user_upload_link`, `create_file_download_link`).

3.  **Specialized Agents (Vertex AI Reasoning Engines):**
    * A team of remote Vertex AI Reasoning Engines (`agents/remote_agents`), each with a specific role:
        * **`gti_agent`:** Connects to Google Threat Intelligence (via `gti-mcp`) to perform IOC analysis on files, IPs, and domains. Deployed using a custom Python script (`setup_alt.py`).
        * **`malware_analysis_agent`:** Performs static analysis on file binaries, using tools like `capstone` to disassemble code and extract suspicious strings.
        * **`incident_response_agent`:** Capable of generating mitigation actions, such as creating Snort rules.
        * **`post_mortem_agent`:** Generates structured incident reports and YARA rules based on the findings.
    * Deployed using a generic Python script (`deploy.py`) invoked by `deploy.sh`.

4.  **File Processing Backend (GCS + Cloud Function):**
    * **Upload Bucket:** A GCS bucket (`soc-upload`, prefixed with project number) receives files from users via secure signed URLs generated by the `chat_agent`.
    * **Cloud Function:** A `soc-file-processor` function (`deployment/file-processor`) is triggered by Eventarc on file upload to the upload bucket. It automatically downloads the file, calculates its SHA256 hash, disassembles the binary using `capstone`, and saves the results.
    * **Download Bucket:** The raw file, its hash (`hash.txt`), and the disassembled code (`disassembled.txt`) are saved to a separate GCS bucket (`soc-download`, prefixed with project number) for secure access by the analysis agents via signed URLs generated by the `chat_agent`.

## Prerequisites

Before you begin, ensure you have the following installed and configured:

* [Python 3.8+](https://www.python.org/downloads/)
* [Google Cloud SDK](https://cloud.google.com/sdk/docs/install) (`gcloud` CLI)
* A Google Cloud Project with the following APIs enabled:
    * Vertex AI API (`aiplatform.googleapis.com`)
    * Cloud Run API (`run.googleapis.com`)
    * Cloud Build API (`cloudbuild.googleapis.com`)
    * Artifact Registry API (`artifactregistry.googleapis.com`)
    * Eventarc API (`eventarc.googleapis.com`)
    * Cloud Functions API (`cloudfunctions.googleapis.com`)
    * Cloud Storage API (`storage.googleapis.com`)
    * IAM API (`iam.googleapis.com`)
* A **Google Threat Intelligence (GTI) API Key**. This is required for the `gti_agent`.
* A user account authenticated via `gcloud` with permissions to manage the resources listed above (e.g., Project Owner, or specific roles like Vertex AI Admin, Cloud Run Admin, Storage Admin, IAM Admin, Service Account User, etc.).

## Installation and Deployment

This project is deployed using a series of shell scripts orchestrated by `deploy_all.sh`. Follow these steps carefully.

### Step 1: Clone the Repository (If necessary)

```bash
git clone <your-repo-url>
cd a2a-sec-deploy
```

### Step 2: Create and Activate a Python Virtual Environment
This isolates the project's dependencies.

```bash
# Create the virtual environment
python3 -m venv venv
# Activate it (macOS/Linux)
source venv/bin/activate
# Or (Windows Command Prompt): 
.\venv\Scripts\activate.bat
# Or (Windows PowerShell): 
.\venv\Scripts\Activate.ps1
```

### Step 3: Install Python Requirements
Install the necessary Python packages for the deployment scripts and agents.

```bash
pip install -r requirements.txt
```

### Step 4: Configure Environment (.env)
Copy the example configuration file and edit it to match your environment.

```bash
cp .env.example .env
```

Now, open `.env` in a text editor and fill in all the variables under `# --- User-Provided Variables ---`:

* `GOOGLE_CLOUD_PROJECT`: Your Google Cloud Project ID.
* `GTI_API_KEY`: Your Google Threat Intelligence API Key.
* `REGION`: The GCP region for deployment (e.g., `us-central1`).
* `SERVICE_NAME`: (Optional) Name for the Cloud Run frontend.
* `REPOSITORY`: (Optional) Name for the Artifact Registry repository.
* `FILE_HANDLER_SERVICE_ACCOUNT_NAME`: (Optional) Name for the file processor SA.
* `UPLOAD_BUCKET`: (Optional) Base name for the upload GCS bucket (project number will be prefixed).
* `DOWNLOAD_BUCKET`: (Optional) Base name for the download GCS bucket (project number will be prefixed).

Do not edit the variables under `# --- Script-Managed Variables ---`. These will be populated automatically by the deployment script.

### Step 5: Authenticate Google Cloud SDK
Log in to `gcloud` and set up Application Default Credentials (ADC).

```bash
# Opens a browser to log in to your Google Account
gcloud auth login
# Authenticates your local application environment (Python scripts)
gcloud auth application-default login
```

### Step 6: Grant Initial IAM Roles (If needed)
The deployment script requires your authenticated user to have permissions to create and manage various resources. If you haven't granted these before, run the following (replace `[YOUR_EMAIL_ACCOUNT]` with the email you used in `gcloud auth login`):

```bash
export GOOGLE_CLOUD_PROJECT=$(gcloud config get-value project) # Or set directly
export USER_EMAIL=$(gcloud config get-value account)

# Grant roles needed for deployment (adjust if necessary based on least privilege)
gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member="user:$USER_EMAIL" --role="roles/storage.admin"
gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member="user:$USER_EMAIL" --role="roles/aiplatform.admin"
gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member="user:$USER_EMAIL" --role="roles/run.admin"
gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member="user:$USER_EMAIL" --role="roles/iam.serviceAccountAdmin"
gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member="user:$USER_EMAIL" --role="roles/eventarc.admin"
gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member="user:$USER_EMAIL" --role="roles/cloudfunctions.admin" # For Cloud Function deployment
gcloud projects add-iam-policy-binding $GOOGLE_CLOUD_PROJECT --member="user:$USER_EMAIL" --role="roles/artifactregistry.admin" # For Artifact Registry

# The user also needs roles/iam.serviceAccountUser on the SAs created by the script,
# but deploy_backend_file_handling.sh grants this during execution.
```

### Step 7: Run the Master Deployment Script
This script coordinates all deployment steps. It must be run using `source` so that environment variables (like agent resource names) are correctly passed between steps and saved to your `.env` file.

```bash
# Make the script executable (if needed)
chmod +x deploy_all.sh deploy.sh deploy_backend_file_handling.sh agents/remote_agents/gti_agent/installation_scripts/install.sh
# Run the master script using "source"
source ./deploy_all.sh
```

The script will proceed through 6 parts, deploying the frontend, all agents, and the file handling backend. It will print progress and save resource names to your `.env` file.
Upon successful completion, the public URL of your frontend service will be printed.

### Resuming Deployment (Optional)
If the deployment script fails midway, you can attempt to resume it from a specific part using the `--start-from` flag (remember to use `source`):

```bash
source ./deploy_all.sh --start-from <part_number>
# Example: Start from deploying tooling agents (Part 3)
source ./deploy_all.sh --start-from 3
```

Project Structure (Relevant Files)
```
.
â”œâ”€â”€ .env                  # Your local configuration (created from .env.example)
â”œâ”€â”€ .env.example          # Template for the environment file
â”œâ”€â”€ deploy_all.sh         # Master deployment script (run this)
â”œâ”€â”€ deploy.sh             # Helper: Deploys a single generic agent via deploy.py
â”œâ”€â”€ deploy.py             # Helper: Python logic for Vertex AI deployment
â”œâ”€â”€ deploy_backend_file_handling.sh # Helper: Deploys GCS buckets & Cloud Function
â”œâ”€â”€ update_env.py         # Helper: Utility to write to the .env file
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ cleanup_deployment.sh # Script to delete all deployed resources
â”‚
â”œâ”€â”€ a2a_demo_front/
â”‚   â”œâ”€â”€ agents/chat_agent/  # Source for the Orchestrator Agent
â”‚   â””â”€â”€ front/              # Source for the Cloud Run Frontend
â”‚
â”œâ”€â”€ agents/remote_agents/ # Source for Specialized Agents
â”‚   â”œâ”€â”€ gti_agent/        # GTI Agent (includes custom setup_alt.py)
â”‚   â”œâ”€â”€ malware_analysis_agent/
â”‚   â”œâ”€â”€ post_mortem_agent/
â”‚   â””â”€â”€ incident_response_agent/
â”‚
â””â”€â”€ deployment/file-processor/ # Source for the File Processing Cloud Function
```
ðŸ§¹ Cleaning Up
To delete all the Google Cloud resources created by this deployment, use the provided cleanup script. This script reads the resource names from your .env file.
WARNING: This action is irreversible and will delete all deployed services, agents, buckets (and their contents), and service accounts.

Make the script executable (if needed):
Bash

chmod +x cleanup_deployment.sh
Run the script:
Bash

./cleanup_deployment.sh
Confirm the Prompt: Carefully review the list of resources that will be deleted and type y to confirm.
The script will then proceed to delete each resource using gcloud commands.
    
